"""
New York Post news source engine
"""

from typing import Dict, Any, List
import logging
from urllib.parse import urljoin
import re
from ..base import SourceEngine, MediaExtractor, ContentValidator

logger = logging.getLogger(__name__)


class NYPostMediaExtractor(MediaExtractor):
    """–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –º–µ–¥–∏–∞ –¥–ª—è NY Post"""
    
    def extract_images(self, url: str, content: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ NY Post"""
        images = []
        
        if 'images' in content:
            for img_url in content['images']:
                if self.validate_image_url(img_url):
                    images.append(img_url)
        
        return images
    
    def extract_videos(self, url: str, content: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∏–¥–µ–æ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ NY Post"""
        videos = []
        
        if 'videos' in content:
            for vid_url in content['videos']:
                if self.validate_video_url(vid_url):
                    videos.append(vid_url)
        
        return videos
    
    def get_fallback_images(self, title: str) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è NY Post"""
        title_lower = title.lower()
        
        # –ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–º—ã
        if any(word in title_lower for word in ['congress', 'senate', 'house', 'capitol', 'politics', 'election']):
            return ['https://images.unsplash.com/photo-1529107386315-e1a2ed48a620?w=1280&h=720&fit=crop']
        
        # –ë–∏–∑–Ω–µ—Å —Ç–µ–º—ã
        elif any(word in title_lower for word in ['business', 'wall street', 'economy', 'market', 'finance']):
            return ['https://images.unsplash.com/photo-1486406146926-c627a92ad1ab?w=1280&h=720&fit=crop']
        
        # –ö—Ä–∏–º–∏–Ω–∞–ª
        elif any(word in title_lower for word in ['crime', 'police', 'arrest', 'shooting', 'murder']):
            return ['https://images.unsplash.com/photo-1532292994-3c4e6e3ab3b9?w=1280&h=720&fit=crop']
        
        # –ù—å—é-–ô–æ—Ä–∫
        elif any(word in title_lower for word in ['new york', 'nyc', 'manhattan', 'brooklyn']):
            return ['https://images.unsplash.com/photo-1496442226666-8d4d0e62e6e9?w=1280&h=720&fit=crop']
        
        # –û–±—â–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞
        else:
            return ['https://images.unsplash.com/photo-1504711434969-e33886168f5c?w=1280&h=720&fit=crop']


class NYPostContentValidator(ContentValidator):
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è NY Post"""
    
    def validate_quality(self, content: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ NY Post"""
        errors = self.get_validation_errors(content)
        
        if errors:
            logger.warning(f"–ö–æ–Ω—Ç–µ–Ω—Ç NY Post –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é: {', '.join(errors)}")
            return False
        
        return True


class NYPostEngine(SourceEngine):
    """
    –î–≤–∏–∂–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π New York Post
    """
    
    def __init__(self, config: Dict[str, Any]):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞ NY Post"""
        super().__init__(config)
        self.media_extractor = NYPostMediaExtractor(config)
        self.content_validator = NYPostContentValidator(config)
    
    def _get_source_name(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        return "New York Post"
    
    def _get_supported_domains(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–º–µ–Ω—ã"""
        return ['nypost.com', 'www.nypost.com']
    
    def can_handle(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å URL"""
        return any(domain in url.lower() for domain in self.supported_domains)
    
    def parse_url(self, url: str, driver=None) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏—Ç URL NY Post –∏—Å–ø–æ–ª—å–∑—É—è Selenium
        """
        logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ NY Post URL: {url[:50]}...")
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            logger.info("üîç Selenium –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
            selenium_result = self._parse_nypost_selenium(url)
            logger.info(f"üîç Selenium —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {selenium_result}")
            
            if selenium_result and selenium_result.get('title'):
                logger.info(f"‚úÖ Selenium –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω: {selenium_result['title'][:50]}...")
                logger.info(f"üìÑ Selenium –∫–æ–Ω—Ç–µ–Ω—Ç: {len(selenium_result.get('content', ''))} —Å–∏–º–≤–æ–ª–æ–≤")
                
                return {
                    'title': selenium_result.get('title', ''),
                    'description': selenium_result.get('description', ''),
                    'content': selenium_result.get('content', ''),
                    'images': selenium_result.get('images', []),
                    'videos': selenium_result.get('videos', []),
                    'published': selenium_result.get('published', ''),
                    'author': selenium_result.get('author', ''),
                    'source': 'New York Post',
                    'content_type': 'news_article'
                }
            else:
                logger.warning("‚ùå Selenium –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è")
                return {}
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ NY Post URL: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return {}
    
    def _parse_nypost_selenium(self, url: str) -> Dict[str, Any]:
        """Selenium –ø–∞—Ä—Å–∏–Ω–≥ NY Post"""
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from bs4 import BeautifulSoup
            import time
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Chrome –¥–ª—è –æ–±—Ö–æ–¥–∞ –∞–Ω—Ç–∏–±–æ—Ç –∑–∞—â–∏—Ç—ã
            chrome_options = Options()
            chrome_options.add_argument('--headless=new')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-software-rasterizer')
            chrome_options.add_argument('--disable-webgl')
            chrome_options.add_argument('--disable-webgl2')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-plugins')
            
            # –£—Å–∫–æ—Ä–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏
            chrome_options.add_argument('--blink-settings=imagesEnabled=false')  # –û—Ç–∫–ª—é—á–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            chrome_options.add_argument('--disable-javascript-harmony')
            
            # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π User-Agent (—Å–≤–µ–∂–∏–π Chrome)
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36')
            
            # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
            chrome_options.add_experimental_option('excludeSwitches', ['enable-automation', 'enable-logging'])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±—Ä–∞—É–∑–µ—Ä–∞
            chrome_options.add_experimental_option('prefs', {
                'profile.default_content_setting_values': {
                    'notifications': 2,
                    'geolocation': 2,
                    'images': 2,  # –ë–ª–æ–∫–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                }
            })
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–≥—Ä—É–∑–∫–∏: eager (–Ω–µ –∂–¥–µ–º –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ä–µ—Å—É—Ä—Å–æ–≤)
            chrome_options.page_load_strategy = 'eager'
            
            driver = webdriver.Chrome(options=chrome_options)
            
            try:
                # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã –¥–ª—è NY Post (—Å–∞–π—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã–º)
                driver.set_page_load_timeout(60)  # –£–≤–µ–ª–∏—á–µ–Ω –¥–æ 60 —Å–µ–∫—É–Ω–¥
                driver.set_script_timeout(20)
                
                # –°–∫—Ä—ã–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ WebDriver
                driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                    'source': '''
                        Object.defineProperty(navigator, 'webdriver', {
                            get: () => undefined
                        });
                        Object.defineProperty(navigator, 'plugins', {
                            get: () => [1, 2, 3, 4, 5]
                        });
                        Object.defineProperty(navigator, 'languages', {
                            get: () => ['en-US', 'en']
                        });
                    '''
                })
                
                # Retry –º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                max_retries = 2
                retry_count = 0
                page_loaded = False
                
                while retry_count < max_retries and not page_loaded:
                    try:
                        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                        if retry_count > 0:
                            logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ({retry_count + 1}/{max_retries})...")
                        else:
                            logger.info(f"üåê –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url[:60]}...")
                        
                        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
                        try:
                            driver.get(url)
                        except Exception as timeout_error:
                            # –ï—Å–ª–∏ —Ç–∞–π–º–∞—É—Ç, –ø—ã—Ç–∞–µ–º—Å—è –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ, —á—Ç–æ –µ—Å—Ç—å
                            if 'timeout' in str(timeout_error).lower():
                                logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ...")
                                try:
                                    driver.execute_script("window.stop();")
                                except:
                                    pass
                            else:
                                raise
                        
                        page_loaded = True
                        
                        # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        logger.info("‚è≥ –û–∂–∏–¥–∞–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
                        time.sleep(5)  # –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ JavaScript
                        
                    except Exception as load_error:
                        retry_count += 1
                        if retry_count >= max_retries:
                            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {str(load_error)[:200]}")
                            raise
                        else:
                            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏, –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞... ({retry_count}/{max_retries})")
                            time.sleep(2)
                
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞ –î–û —Ç–æ–≥–æ, –∫–∞–∫ –æ–Ω –º–æ–∂–µ—Ç –∫—Ä–∞—à–Ω—É—Ç—å—Å—è
                # –ü–æ–ª—É—á–∞–µ–º title –°–†–ê–ó–£, –ø–æ–∫–∞ –±—Ä–∞—É–∑–µ—Ä –∂–∏–≤
                page_title = driver.title
                logger.info(f"üìã Page title: {page_title[:60]}...")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ–ª—É—á–∏–ª–∏ –ª–∏ –º—ã —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—à–∏–±–∫–∏ Chrome
                if '–Ω–µ —É–¥–∞–µ—Ç—Å—è' in page_title.lower() or 'site can' in page_title.lower() or 'this site' in page_title.lower():
                    logger.error(f"‚ùå –ü–æ–ª—É—á–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ—à–∏–±–∫–∏ –±—Ä–∞—É–∑–µ—Ä–∞: {page_title}")
                    logger.error("‚ùå NY Post –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –¥–æ—Å—Ç—É–ø –∏–ª–∏ —Å–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                    return {}
                
                # –ü–æ–ª—É—á–∞–µ–º HTML
                html = driver.page_source
                logger.info(f"üìÑ HTML –¥–ª–∏–Ω–∞: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
                
            finally:
                # –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –°–†–ê–ó–£ –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è HTML
                try:
                    driver.quit()
                    logger.info("‚úÖ –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç")
                except:
                    pass
            
            # –¢–µ–ø–µ—Ä—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º HTML –ë–ï–ó –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –±—Ä–∞—É–∑–µ—Ä–∞
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = ""
            title_selectors = [
                'h1.single-headline',
                'h1.headline',
                'h1[class*="headline"]',
                'h1.entry-heading',
                'h1',
                'meta[property="og:title"]'
            ]
            
            for selector in title_selectors:
                try:
                    if 'meta' in selector:
                        title_elem = soup.select_one(selector)
                        if title_elem:
                            title = title_elem.get('content', '').strip()
                    else:
                        title_elem = soup.select_one(selector)
                        if title_elem:
                            title = title_elem.get_text().strip()
                    
                    if title:
                        logger.info(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ '{selector}': {title[:50]}...")
                        break
                except:
                    pass

            # –û—á–∏—Å—Ç–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –æ—Ç —Å–ª—É–∂–µ–±–Ω—ã—Ö —Ö–≤–æ—Å—Ç–æ–≤
            if title:
                cleanup_patterns = [
                    r"\s*\|\s*New York Post\s*$",
                    r"\s*-\s*New York Post\s*$",
                    r"\s*‚Äì\s*New York Post\s*$",
                    r"\s*\|\s*NY Post\s*$",
                    r"\s*-\s*NY Post\s*$",
                ]
                for pat in cleanup_patterns:
                    title = re.sub(pat, "", title, flags=re.IGNORECASE)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            description = ""
            desc_selectors = [
                'h2.subtitle',
                'h2.dek',
                'div.entry-content-description',
                'meta[property="og:description"]',
                'meta[name="description"]'
            ]
            
            for selector in desc_selectors:
                try:
                    if 'meta' in selector:
                        desc_elem = soup.select_one(selector)
                        if desc_elem:
                            description = desc_elem.get('content', '').strip()
                    else:
                        desc_elem = soup.select_one(selector)
                        if desc_elem:
                            description = desc_elem.get_text().strip()
                    
                    if description:
                        logger.info(f"‚úÖ –û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ '{selector}'")
                        break
                except:
                    pass
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–≤—Ç–æ—Ä–∞
            author = ""
            author_selectors = [
                'div.author-byline a',
                'p.byline a',
                'span.author',
                'a[rel="author"]',
                'meta[name="author"]'
            ]
            
            for selector in author_selectors:
                try:
                    if 'meta' in selector:
                        author_elem = soup.select_one(selector)
                        if author_elem:
                            author = author_elem.get('content', '').strip()
                    else:
                        author_elem = soup.select_one(selector)
                        if author_elem:
                            author = author_elem.get_text().strip()
                    
                    if author:
                        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç "By "
                        author = re.sub(r'^By\s+', '', author, flags=re.IGNORECASE)
                        logger.info(f"‚úÖ –ê–≤—Ç–æ—Ä –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ '{selector}': {author}")
                        break
                except:
                    pass
                
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            published = ""
            date_selectors = [
                'time[datetime]',
                'meta[property="article:published_time"]',
                'p.byline time',
                'span.timestamp'
            ]
            
            for selector in date_selectors:
                try:
                    date_elem = soup.select_one(selector)
                    if date_elem:
                        if selector.startswith('meta'):
                            published = date_elem.get('content', '').strip()
                        elif selector == 'time[datetime]':
                            published = date_elem.get('datetime', '').strip()
                        else:
                            published = date_elem.get_text().strip()
                        
                        if published:
                            logger.info(f"‚úÖ –î–∞—Ç–∞ –Ω–∞–π–¥–µ–Ω–∞ —á–µ—Ä–µ–∑ '{selector}': {published}")
                            break
                except:
                    pass
            
            # –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é
            if not published:
                from datetime import datetime
                published = datetime.now().isoformat()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
            article_text = ""
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ NY Post
            content_selectors = [
                'div.entry-content.single-content p',
                'div.single-content p',
                'div.entry-content p',
                'div[class*="article-content"] p',
                'div[class*="entry-content"] p',
                'article p'
            ]
            
            article_paragraphs = []
            
            for selector in content_selectors:
                try:
                    content_elem = soup.select_one(selector.split(' p')[0])
                    if content_elem:
                        paragraphs = content_elem.find_all('p')
                        for p in paragraphs:
                            text = p.get_text().strip()
                            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                            if (text and 
                                len(text) > 20 and
                                not any(skip in text.lower() for skip in [
                                    'advertisement', 'subscribe', 'newsletter', 
                                    'filed under', 'read next', 'explore more',
                                    'recommended', 'trending', 'share this',
                                    'facebook', 'twitter', 'instagram'
                                ])):
                                article_paragraphs.append(text)
                        
                        if len(article_paragraphs) > 3:
                            logger.info(f"‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ '{selector}': {len(article_paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
                            break
                except:
                    pass
            
            # –ï—Å–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ –¥–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            if not article_paragraphs:
                logger.info("‚ö†Ô∏è –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ –¥–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã")
                paragraphs = soup.find_all('p')
                for p in paragraphs:
                    text = p.get_text().strip()
                    if (text and 
                        len(text) > 20 and
                        not any(skip in text.lower() for skip in [
                            'advertisement', 'subscribe', 'newsletter', 
                            'cookie', 'privacy', 'filed under',
                            'read next', 'explore more', 'recommended'
                        ])):
                        article_paragraphs.append(text)
            
            article_text = ' '.join(article_paragraphs)
            logger.info(f"üìÑ –°–æ–±—Ä–∞–Ω–æ {len(article_paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, –æ–±—â–∞—è –¥–ª–∏–Ω–∞: {len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images: List[str] = []

            def add_image(u: str):
                if not u:
                    return
                full = urljoin(url, u)
                if full not in images:
                    images.append(full)

            # –°–Ω–∞—á–∞–ª–∞ meta tags
            og_img = soup.select_one('meta[property="og:image"]')
            if og_img and og_img.get('content'):
                add_image(og_img.get('content').strip())
            
            tw_img = soup.select_one('meta[name="twitter:image"], meta[name="twitter:image:src"]')
            if tw_img and tw_img.get('content'):
                add_image(tw_img.get('content').strip())

            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏
            article_el = soup.select_one('article') or soup.select_one('div.entry-content') or soup
            for img in article_el.select('img')[:5]:
                src = img.get('src') or img.get('data-src') or img.get('data-lazy-src') or ''
                if not src and img.get('srcset'):
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ (–æ–±—ã—á–Ω–æ —Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ)
                    parts = [p.strip() for p in img.get('srcset').split(',') if p.strip()]
                    if parts:
                        src = parts[-1].split()[0]
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                if src:
                    src_lower = src.lower()
                    if any(skip in src_lower for skip in ['logo', 'icon', 'avatar', 'favicon', 'sprite', 'tracking']):
                        continue
                    add_image(src)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
            def score_image(u: str) -> int:
                s = u.lower()
                score = 0
                if any(size in s for size in ['1200', '1920', '2000', 'large', '1024']):
                    score += 100
                if 'nypost.com' in s:
                    score += 40
                if any(kw in s for kw in ['feature', 'hero', 'main']):
                    score += 30
                if any(skip in s for skip in ['logo', 'icon', 'favicon', 'sprite', 'thumbnail', 'tracking']):
                    score -= 80
                if s.endswith('.jpg') or '.jpg' in s:
                    score += 5
                return score

            images = sorted(list(dict.fromkeys(images)), key=score_image, reverse=True)
            
            logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            for i, img in enumerate(images[:3], 1):
                logger.info(f"  üì∏ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i}: {img[:100]}...")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∏–¥–µ–æ
            videos: List[str] = []
            
            # –°–ø–∏—Å–æ–∫ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            ad_domains = [
                'blob:',  # JavaScript blob URLs - –Ω–µ —Å–∫–∞—á–∏–≤–∞–µ–º—ã–µ
                'flashtalking.com',  # –†–µ–∫–ª–∞–º–∞
                'doubleclick.net',  # Google Ads
                'googlesyndication.com',  # Google Ads
                'googleadservices.com',  # Google Ads
                'amazon-adsystem.com',  # Amazon Ads
                'ads.yahoo.com',  # Yahoo Ads
                'advertising.com',  # AOL Ads
                'adnxs.com',  # AppNexus
                'outbrain.com',  # Outbrain
                'taboola.com',  # Taboola
            ]
            
            def is_ad_video(video_url: str) -> bool:
                """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL —Ä–µ–∫–ª–∞–º–Ω—ã–º –≤–∏–¥–µ–æ"""
                url_lower = video_url.lower()
                return any(ad_domain in url_lower for ad_domain in ad_domains)
            
            # –ò—â–µ–º –≤–∏–¥–µ–æ YouTube, Vimeo (–æ–±—ã—á–Ω–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –Ω–µ —Ä–µ–∫–ª–∞–º–∞)
            for iframe in soup.find_all('iframe'):
                src = iframe.get('src', '')
                if src and any(vid in src for vid in ['youtube', 'vimeo', 'jwplayer']):
                    if not is_ad_video(src):
                        videos.append(src)
                    else:
                        logger.info(f"üö´ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–µ–∫–ª–∞–º–Ω–æ–µ –≤–∏–¥–µ–æ: {src[:80]}...")

            logger.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ {len(videos)} –≤–∏–¥–µ–æ (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–∫–ª–∞–º—ã)")

            return {
                'title': title,
                'description': description,
                'content': article_text,
                'published': published,
                'author': author,
                'images': images,
                'videos': videos
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Selenium –ø–∞—Ä—Å–∏–Ω–≥–∞ NY Post: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return {}
    
    def extract_media(self, url: str, content: Dict[str, Any]) -> Dict[str, List[str]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ–¥–∏–∞ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        images = content.get('images', []) or []
        videos = content.get('videos', []) or []
        logger.info(f"üì∏ NY Post media for this URL: images={len(images)}, videos={len(videos)}")
        return {'images': images, 'videos': videos}
    
    def validate_content(self, content: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–∫—Ç—ã
        if not self.content_validator.validate_facts(content):
            logger.warning("–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø—Ä–æ—à–µ–ª –ø—Ä–æ–≤–µ—Ä–∫—É —Ñ–∞–∫—Ç–æ–≤")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ–¥–∏–∞
        images = content.get('images', [])
        videos = content.get('videos', [])
        
        if not images and not videos:
            logger.warning("‚ùå NY Post –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –∏–º–µ–µ—Ç –º–µ–¥–∏–∞ - –±—Ä–∞–∫—É–µ–º")
            return False
        
        logger.info(f"‚úÖ NY Post –∫–æ–Ω—Ç–µ–Ω—Ç –∏–º–µ–µ—Ç –º–µ–¥–∏–∞: {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(videos)} –≤–∏–¥–µ–æ")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title = content.get('title', '')
        if not self.content_validator.validate_title(title):
            logger.warning("–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é: –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
            return False
        
        # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—É—Å—Ç–æ–µ, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –µ–≥–æ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        description = content.get('description', '').strip()
        if not description:
            logger.info("üìù –û–ø–∏—Å–∞–Ω–∏–µ –ø—É—Å—Ç–æ–µ, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
            content['description'] = f"–ù–æ–≤–æ—Å—Ç—å: {title}"
        
        return True
    
    def get_fallback_media(self, title: str) -> Dict[str, List[str]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –º–µ–¥–∏–∞"""
        images = self.media_extractor.get_fallback_images(title)
        return {
            'images': images,
            'videos': []
        }

