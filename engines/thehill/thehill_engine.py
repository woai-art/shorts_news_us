"""
The Hill news source engine
"""

from typing import Dict, Any, List
import logging
from urllib.parse import urljoin, urlparse
import re
from ..base import SourceEngine, MediaExtractor, ContentValidator

logger = logging.getLogger(__name__)


class TheHillMediaExtractor(MediaExtractor):
    """Ð˜Ð·Ð²Ð»ÐµÐºÐ°Ñ‚ÐµÐ»ÑŒ Ð¼ÐµÐ´Ð¸Ð° Ð´Ð»Ñ The Hill"""
    
    def extract_images(self, url: str, content: Dict[str, Any]) -> List[str]:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° The Hill"""
        images = []
        
        if 'images' in content:
            for img_url in content['images']:
                if self.validate_image_url(img_url):
                    images.append(img_url)
        
        return images
    
    def extract_videos(self, url: str, content: Dict[str, Any]) -> List[str]:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° The Hill"""
        videos = []
        
        if 'videos' in content:
            for vid_url in content['videos']:
                if self.validate_video_url(vid_url):
                    videos.append(vid_url)
        
        return videos
    
    def get_fallback_images(self, title: str) -> List[str]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ fallback Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð´Ð»Ñ The Hill"""
        title_lower = title.lower()
        
        # ÐŸÐ¾Ð»Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ‚ÐµÐ¼Ñ‹ - ÐšÐ°Ð¿Ð¸Ñ‚Ð¾Ð»Ð¸Ð¹
        if any(word in title_lower for word in ['congress', 'senate', 'house', 'capitol', 'representative', 'senator']):
            return ['https://images.unsplash.com/photo-1529107386315-e1a2ed48a620?w=1280&h=720&fit=crop']
        
        # ÐŸÑ€ÐµÐ·Ð¸Ð´ÐµÐ½Ñ‚ÑÐºÐ¸Ðµ Ñ‚ÐµÐ¼Ñ‹
        elif any(word in title_lower for word in ['president', 'biden', 'trump', 'white house']):
            return ['https://images.unsplash.com/photo-1551524164-6cf2ac5313f4?w=1280&h=720&fit=crop']
        
        # Ð’Ñ‹Ð±Ð¾Ñ€Ñ‹
        elif any(word in title_lower for word in ['election', 'campaign', 'vote', 'ballot']):
            return ['https://images.unsplash.com/photo-1541872703-74c3ee0f25b1?w=1280&h=720&fit=crop']
        
        # ÐžÐ±Ñ‰Ð°Ñ Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸ÐºÐ° - Ð’Ð°ÑˆÐ¸Ð½Ð³Ñ‚Ð¾Ð½
        else:
            return ['https://images.unsplash.com/photo-1555596000-aa02ca55b2e8?w=1280&h=720&fit=crop']


class TheHillContentValidator(ContentValidator):
    """Ð’Ð°Ð»Ð¸Ð´Ð°Ñ‚Ð¾Ñ€ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° Ð´Ð»Ñ The Hill"""
    
    def validate_quality(self, content: Dict[str, Any]) -> bool:
        """Ð’Ð°Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÑ‚ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° The Hill"""
        errors = self.get_validation_errors(content)
        
        if errors:
            logger.warning(f"ÐšÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ The Hill Ð½Ðµ Ð¿Ñ€Ð¾ÑˆÐµÐ» Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ: {', '.join(errors)}")
            return False
        
        return True


class TheHillEngine(SourceEngine):
    """
    Ð”Ð²Ð¸Ð¶Ð¾Ðº Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹ The Hill
    """
    
    def __init__(self, config: Dict[str, Any]):
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð²Ð¸Ð¶ÐºÐ° The Hill"""
        super().__init__(config)
        self.media_extractor = TheHillMediaExtractor(config)
        self.content_validator = TheHillContentValidator(config)
    
    def _get_source_name(self) -> str:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°"""
        return "The Hill"
    
    def _get_supported_domains(self) -> List[str]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼Ñ‹Ðµ Ð´Ð¾Ð¼ÐµÐ½Ñ‹"""
        return ['thehill.com', 'www.thehill.com']
    
    def can_handle(self, url: str) -> bool:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚, Ð¼Ð¾Ð¶ÐµÑ‚ Ð»Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ URL"""
        return any(domain in url.lower() for domain in self.supported_domains)
    
    def parse_url(self, url: str, driver=None) -> Dict[str, Any]:
        """
        ÐŸÐ°Ñ€ÑÐ¸Ñ‚ URL The Hill Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Selenium
        """
        logger.info(f"ðŸ” ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ The Hill URL: {url[:50]}...")
        
        try:
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Selenium Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°
            logger.info("ðŸ” Selenium Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ° Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°...")
            selenium_result = self._parse_thehill_selenium(url)
            logger.info(f"ðŸ” Selenium Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {selenium_result}")
            
            if selenium_result and selenium_result.get('title'):
                logger.info(f"âœ… Selenium Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ ÑƒÑÐ¿ÐµÑˆÐµÐ½: {selenium_result['title'][:50]}...")
                logger.info(f"ðŸ“„ Selenium ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚: {len(selenium_result.get('content', ''))} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
                
                return {
                    'title': selenium_result.get('title', ''),
                    'description': selenium_result.get('description', ''),
                    'content': selenium_result.get('content', ''),
                    'images': selenium_result.get('images', []),
                    'videos': selenium_result.get('videos', []),
                    'published': selenium_result.get('published', ''),
                    'source': 'The Hill',
                    'content_type': 'news_article'
                }
            else:
                logger.warning("âŒ Selenium Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð½Ðµ ÑƒÐ´Ð°Ð»ÑÑ")
                return {}
                
        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° The Hill URL: {e}")
            import traceback
            logger.error(f"âŒ Traceback: {traceback.format_exc()}")
            return {}
    
    def _parse_thehill_selenium(self, url: str) -> Dict[str, Any]:
        """Selenium Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ The Hill"""
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from bs4 import BeautifulSoup
            import time
            
            # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Chrome
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            driver = webdriver.Chrome(options=chrome_options)
            
            try:
                driver.get(url)
                time.sleep(3)  # Ð–Ð´ÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
                
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ HTML
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                logger.info(f"ðŸ“„ HTML Ð´Ð»Ð¸Ð½Ð°: {len(html)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
                
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº
                title = ""
                title_selectors = [
                    'h1.headline__text',
                    'h1[class*="headline"]',
                    'h1',
                    'meta[property="og:title"]'
                ]
                
                for selector in title_selectors:
                    try:
                        if 'meta' in selector:
                            title_elem = soup.select_one(selector)
                            if title_elem:
                                title = title_elem.get('content', '').strip()
                        else:
                            title_elem = soup.select_one(selector)
                            if title_elem:
                                title = title_elem.get_text().strip()
                        
                        if title:
                            logger.info(f"âœ… Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº Ð½Ð°Ð¹Ð´ÐµÐ½ Ñ‡ÐµÑ€ÐµÐ· '{selector}': {title[:50]}...")
                            break
                    except:
                        pass

                # ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ° Ð¾Ñ‚ ÑÐ»ÑƒÐ¶ÐµÐ±Ð½Ñ‹Ñ… Ñ…Ð²Ð¾ÑÑ‚Ð¾Ð²
                if title:
                    cleanup_patterns = [
                        r"\s*\|\s*The Hill\s*$",
                        r"\s*-\s*The Hill\s*$",
                        r"\s*â€“\s*The Hill\s*$",
                    ]
                    for pat in cleanup_patterns:
                        title = re.sub(pat, "", title, flags=re.IGNORECASE)
                
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ
                description = ""
                desc_selectors = [
                    'p.article__dek',
                    'div.article__dek p',
                    'meta[property="og:description"]',
                    'meta[name="description"]'
                ]
                
                for selector in desc_selectors:
                    try:
                        if 'meta' in selector:
                            desc_elem = soup.select_one(selector)
                            if desc_elem:
                                description = desc_elem.get('content', '').strip()
                        else:
                            desc_elem = soup.select_one(selector)
                            if desc_elem:
                                description = desc_elem.get_text().strip()
                        
                        if description:
                            logger.info(f"âœ… ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ñ‡ÐµÑ€ÐµÐ· '{selector}'")
                            break
                    except:
                        pass
                
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð´Ð°Ñ‚Ñƒ Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
                published = ""
                date_selectors = [
                    'time[datetime]',
                    'meta[property="article:published_time"]',
                    'span[class*="timestamp"]'
                ]
                
                for selector in date_selectors:
                    try:
                        date_elem = soup.select_one(selector)
                        if date_elem:
                            if selector.startswith('meta'):
                                published = date_elem.get('content', '').strip()
                            elif selector == 'time[datetime]':
                                published = date_elem.get('datetime', '').strip()
                            else:
                                published = date_elem.get_text().strip()
                            
                            if published:
                                logger.info(f"âœ… Ð”Ð°Ñ‚Ð° Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ñ‡ÐµÑ€ÐµÐ· '{selector}': {published}")
                                break
                    except:
                        pass
                
                # Ð•ÑÐ»Ð¸ Ð´Ð°Ñ‚Ð° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰ÑƒÑŽ
                if not published:
                    from datetime import datetime
                    published = datetime.now().isoformat()
                
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ ÑÑ‚Ð°Ñ‚ÑŒÐ¸
                article_text = ""
                
                # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð½Ñ‹Ðµ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° The Hill
                content_selectors = [
                    'div.article__text',
                    'div[class*="article-text"]',
                    'div[class*="article-body"]',
                    'article div p'
                ]
                
                article_paragraphs = []
                
                for selector in content_selectors:
                    try:
                        content_elem = soup.select_one(selector)
                        if content_elem:
                            paragraphs = content_elem.find_all('p')
                            for p in paragraphs:
                                text = p.get_text().strip()
                                # Ð˜ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ ÑÐ»ÑƒÐ¶ÐµÐ±Ð½Ñ‹Ðµ Ñ‚ÐµÐºÑÑ‚Ñ‹
                                if (text and 
                                    len(text) > 20 and
                                    not any(skip in text.lower() for skip in ['advertisement', 'subscribe', 'newsletter'])):
                                    article_paragraphs.append(text)
                            
                            if len(article_paragraphs) > 3:
                                logger.info(f"âœ… ÐšÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð½Ð°Ð¹Ð´ÐµÐ½ Ñ‡ÐµÑ€ÐµÐ· '{selector}': {len(article_paragraphs)} Ð¿Ð°Ñ€Ð°Ð³Ñ€Ð°Ñ„Ð¾Ð²")
                                break
                    except:
                        pass
                
                # Ð•ÑÐ»Ð¸ ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡Ð½Ñ‹Ðµ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð½Ðµ Ð´Ð°Ð»Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°, ÑÐ¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð²ÑÐµ Ð¿Ð°Ñ€Ð°Ð³Ñ€Ð°Ñ„Ñ‹
                if not article_paragraphs:
                    logger.info("âš ï¸ Ð¡Ð¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡Ð½Ñ‹Ðµ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð½Ðµ Ð´Ð°Ð»Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°, ÑÐ¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð²ÑÐµ Ð¿Ð°Ñ€Ð°Ð³Ñ€Ð°Ñ„Ñ‹")
                    paragraphs = soup.find_all('p')
                    for p in paragraphs:
                        text = p.get_text().strip()
                        if (text and 
                            len(text) > 20 and
                            not any(skip in text.lower() for skip in ['advertisement', 'subscribe', 'newsletter', 'cookie', 'privacy'])):
                            article_paragraphs.append(text)
                
                article_text = ' '.join(article_paragraphs)
                logger.info(f"ðŸ“„ Ð¡Ð¾Ð±Ñ€Ð°Ð½Ð¾ {len(article_paragraphs)} Ð¿Ð°Ñ€Ð°Ð³Ñ€Ð°Ñ„Ð¾Ð², Ð¾Ð±Ñ‰Ð°Ñ Ð´Ð»Ð¸Ð½Ð°: {len(article_text)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
                
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
                images: List[str] = []

                def add_image(u: str):
                    if not u:
                        return
                    full = urljoin(url, u)
                    if full not in images:
                        images.append(full)

                # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° meta tags
                og_img = soup.select_one('meta[property="og:image"]')
                if og_img and og_img.get('content'):
                    add_image(og_img.get('content').strip())
                
                tw_img = soup.select_one('meta[name="twitter:image"], meta[name="twitter:image:src"]')
                if tw_img and tw_img.get('content'):
                    add_image(tw_img.get('content').strip())

                # Ð˜Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð¸Ð· ÑÑ‚Ð°Ñ‚ÑŒÐ¸
                article_el = soup.select_one('article') or soup
                for img in article_el.select('img')[:5]:
                    src = img.get('src') or img.get('data-src') or img.get('data-lazy-src') or ''
                    if not src and img.get('srcset'):
                        # Ð‘ÐµÑ€ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐµ (Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ ÑÐ°Ð¼Ð¾Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ðµ)
                        parts = [p.strip() for p in img.get('srcset').split(',') if p.strip()]
                        if parts:
                            src = parts[-1].split()[0]
                    
                    # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ðµ Ð¸ ÑÐ»ÑƒÐ¶ÐµÐ±Ð½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
                    if src:
                        src_lower = src.lower()
                        if any(skip in src_lower for skip in ['logo', 'icon', 'avatar', 'favicon', 'sprite']):
                            continue
                        add_image(src)

                # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚Ñƒ
                def score_image(u: str) -> int:
                    s = u.lower()
                    score = 0
                    if any(size in s for size in ['1200', '1920', '2000', 'large']):
                        score += 100
                    if 'thehill.com' in s:
                        score += 40
                    if any(kw in s for kw in ['feature', 'hero', 'main']):
                        score += 30
                    if any(skip in s for skip in ['logo', 'icon', 'favicon', 'sprite', 'thumbnail']):
                        score -= 80
                    if s.endswith('.jpg') or '.jpg' in s:
                        score += 5
                    return score

                images = sorted(list(dict.fromkeys(images)), key=score_image, reverse=True)
                
                logger.info(f"ðŸ“¸ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(images)} Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹")
                for i, img in enumerate(images[:3], 1):
                    logger.info(f"  ðŸ“¸ Ð˜Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ {i}: {img[:100]}...")

                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð²Ð¸Ð´ÐµÐ¾
                videos: List[str] = []
                
                # Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ€ÐµÐºÐ»Ð°Ð¼Ð½Ñ‹Ñ… Ð´Ð¾Ð¼ÐµÐ½Ð¾Ð² Ð´Ð»Ñ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸
                ad_domains = [
                    'blob:',  # JavaScript blob URLs - Ð½Ðµ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÐ¼Ñ‹Ðµ
                    'flashtalking.com',  # Ð ÐµÐºÐ»Ð°Ð¼Ð°
                    'doubleclick.net',  # Google Ads
                    'googlesyndication.com',  # Google Ads
                    'googleadservices.com',  # Google Ads
                    'amazon-adsystem.com',  # Amazon Ads
                    'ads.yahoo.com',  # Yahoo Ads
                    'advertising.com',  # AOL Ads
                    'adnxs.com',  # AppNexus
                    'outbrain.com',  # Outbrain
                    'taboola.com',  # Taboola
                ]
                
                def is_ad_video(video_url: str) -> bool:
                    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ URL Ñ€ÐµÐºÐ»Ð°Ð¼Ð½Ñ‹Ð¼ Ð²Ð¸Ð´ÐµÐ¾"""
                    url_lower = video_url.lower()
                    return any(ad_domain in url_lower for ad_domain in ad_domains)
                
                # Ð˜Ñ‰ÐµÐ¼ Ð²Ð¸Ð´ÐµÐ¾ YouTube, Vimeo (Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ Ð²ÑÑ‚Ñ€Ð¾ÐµÐ½Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚, Ð½Ðµ Ñ€ÐµÐºÐ»Ð°Ð¼Ð°)
                for iframe in soup.find_all('iframe'):
                    src = iframe.get('src', '')
                    if src and any(vid in src for vid in ['youtube', 'vimeo', 'jwplayer']):
                        if not is_ad_video(src):
                            videos.append(src)
                        else:
                            logger.info(f"ðŸš« ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ñ€ÐµÐºÐ»Ð°Ð¼Ð½Ð¾Ðµ Ð²Ð¸Ð´ÐµÐ¾: {src[:80]}...")
                
                # HTML5 Ð²Ð¸Ð´ÐµÐ¾ ÐÐ• Ð˜Ð—Ð’Ð›Ð•ÐšÐÐ•Ðœ Ñ The Hill - Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ ÑÑ‚Ð¾ Ñ€ÐµÐºÐ»Ð°Ð¼Ð°
                # Ð•ÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ñ‹ HTML5 Ð²Ð¸Ð´ÐµÐ¾, Ñ€Ð°ÑÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ ÐºÐ¾Ð´ Ð½Ð¸Ð¶Ðµ
                # for video in soup.find_all('video'):
                #     src = video.get('src', '')
                #     if src and not is_ad_video(src):
                #         videos.append(urljoin(url, src))
                #     for source in video.find_all('source'):
                #         src = source.get('src', '')
                #         if src and not is_ad_video(src):
                #             videos.append(urljoin(url, src))

                logger.info(f"ðŸŽ¬ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(videos)} Ð²Ð¸Ð´ÐµÐ¾ (Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ñ€ÐµÐºÐ»Ð°Ð¼Ñ‹)")

                return {
                    'title': title,
                    'description': description,
                    'content': article_text,
                    'published': published,
                    'images': images,
                    'videos': videos
                }
                
            finally:
                driver.quit()
            
        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Selenium Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° The Hill: {e}")
            import traceback
            logger.error(f"âŒ Traceback: {traceback.format_exc()}")
            return {}
    
    def extract_media(self, url: str, content: Dict[str, Any]) -> Dict[str, List[str]]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¼ÐµÐ´Ð¸Ð° Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°"""
        images = content.get('images', []) or []
        videos = content.get('videos', []) or []
        logger.info(f"ðŸ“¸ The Hill media for this URL: images={len(images)}, videos={len(videos)}")
        return {'images': images, 'videos': videos}
    
    def validate_content(self, content: Dict[str, Any]) -> bool:
        """Ð’Ð°Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚"""
        # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ„Ð°ÐºÑ‚Ñ‹
        if not self.content_validator.validate_facts(content):
            logger.warning("ÐšÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð½Ðµ Ð¿Ñ€Ð¾ÑˆÐµÐ» Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÑƒ Ñ„Ð°ÐºÑ‚Ð¾Ð²")
            return False
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ð¼ÐµÐ´Ð¸Ð°
        images = content.get('images', [])
        videos = content.get('videos', [])
        
        if not images and not videos:
            logger.warning("âŒ The Hill ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð½Ðµ Ð¸Ð¼ÐµÐµÑ‚ Ð¼ÐµÐ´Ð¸Ð° - Ð±Ñ€Ð°ÐºÑƒÐµÐ¼")
            return False
        
        logger.info(f"âœ… The Hill ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð¸Ð¼ÐµÐµÑ‚ Ð¼ÐµÐ´Ð¸Ð°: {len(images)} Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, {len(videos)} Ð²Ð¸Ð´ÐµÐ¾")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº
        title = content.get('title', '')
        if not self.content_validator.validate_title(title):
            logger.warning("ÐšÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð½Ðµ Ð¿Ñ€Ð¾ÑˆÐµÐ» Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ: ÐÐµÐ²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¹ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº")
            return False
        
        # Ð•ÑÐ»Ð¸ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿ÑƒÑÑ‚Ð¾Ðµ, Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÐµÐ³Ð¾ Ð¸Ð· Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°
        description = content.get('description', '').strip()
        if not description:
            logger.info("ðŸ“ ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿ÑƒÑÑ‚Ð¾Ðµ, Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð· Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°")
            content['description'] = f"ÐÐ¾Ð²Ð¾ÑÑ‚ÑŒ: {title}"
        
        return True
    
    def get_fallback_media(self, title: str) -> Dict[str, List[str]]:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ fallback Ð¼ÐµÐ´Ð¸Ð°"""
        images = self.media_extractor.get_fallback_images(title)
        return {
            'images': images,
            'videos': []
        }

