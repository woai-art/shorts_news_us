#!/usr/bin/env python3
"""
ABC News Engine
–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å abcnews.go.com
"""

import logging
import re
from typing import Dict, List, Any
from urllib.parse import urlparse, urljoin, parse_qs
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException

from ..base.source_engine import SourceEngine

logger = logging.getLogger(__name__)

class ABCNewsEngine(SourceEngine):
    """–î–≤–∏–∂–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ ABC News"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
    
    def _get_supported_domains(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–º–µ–Ω—ã"""
        return [
            "abcnews.go.com",
            "www.abcnews.go.com"
        ]
    
    def _get_source_name(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        return "ABC News"
    
    def can_handle(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å URL"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω
        if not any(domain in url.lower() for domain in self.supported_domains):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ entryId - –µ—Å–ª–∏ –µ—Å—Ç—å, —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–ø–∏—Å—å
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)
        has_entry_id = 'entryId' in query_params or 'entryid' in query_params
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å entryId, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–∂–µ live-updates
        if has_entry_id:
            logger.info(f"‚úÖ ABC News: URL —Å–æ–¥–µ—Ä–∂–∏—Ç entryId, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—É—é –∑–∞–ø–∏—Å—å")
            return True
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–∏–ø—ã URL –±–µ–∑ entryId, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        excluded_patterns = [
            '/live-updates/',  # Live updates –±–µ–∑ entryId - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ–º—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        ]
        
        for pattern in excluded_patterns:
            if pattern in url.lower():
                logger.info(f"‚è≠Ô∏è ABC News: URL —Å–æ–¥–µ—Ä–∂–∏—Ç –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω '{pattern}' –±–µ–∑ entryId, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return False
        
        return True
    
    def get_engine_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–≤–∏–∂–∫–µ"""
        return {
            'name': self.source_name,
            'supported_domains': self.supported_domains,
            'version': '1.0.0'
        }
    
    def parse_url(self, url: str, driver=None) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ URL ABC News
        
        Args:
            url: URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            driver: Selenium WebDriver (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç–∏ –∏–ª–∏ None
        """
        try:
            logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ ABC News URL: {url}")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
            if not self._is_valid_url(url):
                logger.warning(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π URL: {url}")
                return None
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Selenium –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if driver:
                return self._parse_with_selenium(url, driver)
            else:
                # –°–æ–∑–¥–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
                return self._parse_with_selenium(url, None)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ ABC News URL {url}: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return None
    
    def _parse_with_selenium(self, url: str, driver=None) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Selenium"""
        driver_created = False
        try:
            # –ï—Å–ª–∏ –¥—Ä–∞–π–≤–µ—Ä –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, —Å–æ–∑–¥–∞–µ–º –µ–≥–æ
            if driver is None:
                from selenium import webdriver
                from selenium.webdriver.chrome.options import Options
                
                chrome_options = Options()
                chrome_options.add_argument('--headless')
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--disable-dev-shm-usage')
                chrome_options.add_argument('--disable-gpu')
                chrome_options.add_argument('--window-size=1920,1080')
                chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
                
                driver = webdriver.Chrome(options=chrome_options)
                driver_created = True
            
            logger.info(f"üîç Selenium –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ live-updates?
            is_live_update = '/live-updates/' in url.lower()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            driver.get(url)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            import time
            time.sleep(5 if is_live_update else 3)  # –ë–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è live-updates
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            title = self._extract_title(driver)
            content = self._extract_content(driver)
            description = self._extract_description(driver, content)
            author = self._extract_author(driver)
            publish_date = self._extract_publish_date(driver)
            images = self._extract_images(driver, url)
            videos = self._extract_videos(driver)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if not self._validate_parsed_content({'title': title, 'content': content}):
                logger.warning("‚ùå –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é")
                return None
            
            result = {
                'title': title,
                'description': description,
                'content': content,
                'author': author,
                'published': publish_date,
                'source': self.source_name,
                'url': url,
                'images': images,
                'videos': videos,
                'content_type': 'article'
            }
            
            logger.info(f"üìù ABC News –ø–∞—Ä—Å–∏–Ω–≥: –∑–∞–≥–æ–ª–æ–≤–æ–∫='{title[:50]}...', –∫–æ–Ω—Ç–µ–Ω—Ç={len(content)} —Å–∏–º–≤–æ–ª–æ–≤, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è={len(images)}, –≤–∏–¥–µ–æ={len(videos)}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Selenium –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return None
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä, –µ—Å–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –µ–≥–æ —Å–∞–º–∏
            if driver_created and driver:
                try:
                    driver.quit()
                except Exception:
                    pass
    
    def _extract_title(self, driver) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_selectors = [
                'h1[data-testid="headline"]',
                'h1.Article__Headline',
                'h1.headline',
                'h1[class*="headline"]',
                'h1[class*="title"]',
                'h1[class*="Headline"]',
                # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è live-updates
                '.LiveBlogPost__Headline h1',
                '.LiveBlogPost__Headline h2',
                '.LiveBlogPost h1',
                '.LiveBlogPost h2',
                '[class*="LiveBlog"] h1',
                '[class*="LiveBlog"] h2',
                'h1',
                'h2',
                '.article-header h1',
                '[data-testid="article-headline"] h1',
                'header h1',
                'article h1'
            ]
            
            for selector in title_selectors:
                try:
                    element = driver.find_element(By.CSS_SELECTOR, selector)
                    title = element.text.strip()
                    if title and len(title) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                        logger.info(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ '{selector}': {title[:50]}...")
                        return title
                except NoSuchElementException:
                    continue
            
            # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ meta —Ç–µ–≥–æ–≤
            try:
                meta_title = driver.find_element(By.CSS_SELECTOR, 'meta[property="og:title"]')
                title = meta_title.get_attribute('content').strip()
                if title and len(title) > 10:
                    logger.info(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–π–¥–µ–Ω –∏–∑ og:title: {title[:50]}...")
                    return title
            except NoSuchElementException:
                pass
            
            logger.warning("‚ö†Ô∏è –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return "ABC News Article"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞: {e}")
            return "ABC News Article"
    
    def _extract_description(self, driver, content: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è/—Ä–µ–∑—é–º–µ
            desc_selectors = [
                'meta[property="og:description"]',
                'meta[name="description"]',
                'p.Article__Description',
                'p.summary',
                '.article-description',
                '[data-testid="article-description"]'
            ]
            
            for selector in desc_selectors:
                try:
                    if selector.startswith('meta'):
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        description = element.get_attribute('content').strip()
                    else:
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        description = element.text.strip()
                    
                    if description and len(description) > 20:
                        logger.info(f"‚úÖ –û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ: {description[:50]}...")
                        return description
                except NoSuchElementException:
                    continue
            
            # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if content and len(content) > 50:
                description = content[:500] + '...' if len(content) > 500 else content
                logger.info(f"‚úÖ –û–ø–∏—Å–∞–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {description[:50]}...")
                return description
            
            logger.warning("‚ö†Ô∏è –û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è: {e}")
            return ""
    
    def _extract_content(self, driver) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            content_parts = []
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_selectors = [
                '.Article__Content p',
                '.article-body p',
                '.article-content p',
                '[data-testid="article-body"] p',
                '.Article__Body p',
                'article .Article__Content p',
                # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è live-updates
                '.LiveBlogPost__Content p',
                '.LiveBlogPost__Body p',
                '[class*="LiveBlogPost"] p',
                '[class*="LiveBlog"] p',
                # –ë–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å—Ç–∞—Ç–µ–π
                'main article p',
                'main p',
                '[role="main"] p',
                'div[class*="Article"] p',
                'div[class*="Story"] p',
                'section[class*="body"] p',
                'article p',
                '.story-body p',
                # –°–∞–º—ã–π –æ–±—â–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä (–ø–æ—Å–ª–µ–¥–Ω–∏–π)
                'body p'
            ]
            
            for selector in content_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(elements)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä '{selector}'")
                        for element in elements:
                            text = element.text.strip()
                            if text and len(text) > 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∞–±–∑–∞—Ü–∞
                                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                                if not self._is_service_text(text):
                                    content_parts.append(text)
                        break  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç, –≤—ã—Ö–æ–¥–∏–º
                except NoSuchElementException:
                    continue
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ —Å—Ç–∞—Ç—å–µ
            if not content_parts:
                try:
                    article = driver.find_element(By.TAG_NAME, "article")
                    paragraphs = article.find_elements(By.TAG_NAME, "p")
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ article")
                    for p in paragraphs:
                        text = p.text.strip()
                        if text and len(text) > 20:
                            if not self._is_service_text(text):
                                content_parts.append(text)
                except NoSuchElementException:
                    pass
            
            content = ' '.join(content_parts)
            
            if content:
                logger.info(f"‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç –∏–∑–≤–ª–µ—á–µ–Ω: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {len(content_parts)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
                return content
            else:
                logger.warning("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return ""
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return ""
    
    def _is_service_text(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Å–ª—É–∂–µ–±–Ω—ã–º"""
        service_keywords = [
            'subscribe',
            'newsletter',
            'advertisement',
            'click here',
            'read more',
            'related:',
            'follow us',
            'sign up',
            'trending',
            'sponsored',
            'editor\'s note',
            'popular reads',
            'sponsored content',
            'by taboola',
            'watch live',
            'stream on',
            'shop',
            'interest successfully added',
            'turn on desktop notifications',
            'we\'ll notify you',
            'related topics',
            'abc news network',
            'privacy policy',
            'terms of use',
            'contact us',
            '¬© 20',  # Copyright
            'all rights reserved'
        ]
        
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if any(keyword in text_lower for keyword in service_keywords):
            return True
        
        # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã (–º–µ–Ω—å—à–µ 30 —Å–∏–º–≤–æ–ª–æ–≤) - –≤–µ—Ä–æ—è—Ç–Ω–æ –º–µ–Ω—é/—Å—Å—ã–ª–∫–∏
        if len(text) < 30:
            return True
        
        # –¢–µ–∫—Å—Ç—ã, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ —Ü–∏—Ñ—Ä –∏ —Å–∏–º–≤–æ–ª–æ–≤
        if text.replace(' ', '').replace(',', '').replace('.', '').replace(':', '').isdigit():
            return True
            
        return False
    
    def _extract_author(self, driver) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞"""
        try:
            author_selectors = [
                '[data-testid="byline"]',
                '.byline',
                '.article-byline',
                '.author',
                '[class*="byline"]',
                '[class*="author"]',
                '.Article__Author',
                'meta[name="author"]'
            ]
            
            for selector in author_selectors:
                try:
                    if selector.startswith('meta'):
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        author = element.get_attribute('content').strip()
                    else:
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        author = element.text.strip()
                    
                    if author and len(author) > 2:
                        # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–µ–≥–æ ("By ", "ABC News" –∏ —Ç.–¥.)
                        author = re.sub(r'^By\s+', '', author, flags=re.IGNORECASE)
                        logger.info(f"‚úÖ –ê–≤—Ç–æ—Ä –Ω–∞–π–¥–µ–Ω: {author}")
                        return author
                except NoSuchElementException:
                    continue
            
            return "ABC News"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–∞: {e}")
            return "ABC News"
    
    def _extract_publish_date(self, driver) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        try:
            date_selectors = [
                '[data-testid="timestamp"]',
                '.timestamp',
                '.article-timestamp',
                'time[datetime]',
                '[class*="timestamp"]',
                '[class*="date"]',
                '.Article__Date',
                'meta[property="article:published_time"]'
            ]
            
            for selector in date_selectors:
                try:
                    element = driver.find_element(By.CSS_SELECTOR, selector)
                    
                    if selector.startswith('meta'):
                        date_text = element.get_attribute('content').strip()
                    elif selector == 'time[datetime]':
                        date_text = element.get_attribute('datetime').strip()
                    else:
                        date_text = element.text.strip()
                    
                    if date_text:
                        logger.info(f"‚úÖ –î–∞—Ç–∞ –Ω–∞–π–¥–µ–Ω–∞: {date_text}")
                        return date_text
                except NoSuchElementException:
                    continue
            
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞—Ç—ã: {e}")
            return ""
    
    def _extract_images(self, driver, url: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            images = []
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ meta —Ç–µ–≥–æ–≤
            meta_img_selectors = [
                'meta[property="og:image"]',
                'meta[name="twitter:image"]',
                'meta[property="twitter:image"]'
            ]
            
            for selector in meta_img_selectors:
                try:
                    element = driver.find_element(By.CSS_SELECTOR, selector)
                    img_url = element.get_attribute('content')
                    if img_url and self._is_valid_image_url(img_url):
                        full_url = urljoin(url, img_url)
                        if full_url not in images:
                            images.append(full_url)
                            logger.debug(f"üì∏ –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ meta: {full_url}")
                except NoSuchElementException:
                    continue
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            img_selectors = [
                '.Article__Content img',
                '.article-body img',
                '.article-content img',
                '[data-testid="article-body"] img',
                'article img',
                '.Article__Hero img',
                'figure img',
                '.image-container img',
                '.media img',
                'picture img'
            ]
            
            for selector in img_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for img in elements:
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                        src = img.get_attribute('src')
                        data_src = img.get_attribute('data-src')
                        data_lazy = img.get_attribute('data-lazy')
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                        for img_url in [src, data_src, data_lazy]:
                            if img_url and self._is_valid_image_url(img_url):
                                # –î–µ–ª–∞–µ–º URL –ø–æ–ª–Ω—ã–º
                                full_url = urljoin(url, img_url)
                                if full_url not in images:
                                    images.append(full_url)
                                    logger.debug(f"üì∏ –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {full_url}")
                except NoSuchElementException:
                    continue
            
            logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            return images
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
            return []
    
    def _extract_videos(self, driver) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ"""
        try:
            videos = []
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –≤–∏–¥–µ–æ
            video_selectors = [
                '.Article__Content video',
                '.article-body video',
                '.article-content video',
                '[data-testid="article-body"] video',
                'article video',
                'iframe[src*="youtube"]',
                'iframe[src*="vimeo"]',
                'iframe[src*="abcnews"]',
                'video source',
                '.video-container video',
                '.media video',
                '.Article__Video video'
            ]
            
            for selector in video_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for video in elements:
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                        src = video.get_attribute('src')
                        data_src = video.get_attribute('data-src')
                        
                        # –î–ª—è source —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                        if video.tag_name == 'source':
                            src = video.get_attribute('src')
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                        for video_url in [src, data_src]:
                            if video_url and self._is_valid_video_url(video_url):
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –ø–æ–ª–Ω—ã–π
                                if video_url.startswith('http'):
                                    if video_url not in videos:
                                        videos.append(video_url)
                                        logger.debug(f"üé¨ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤–∏–¥–µ–æ: {video_url}")
                                else:
                                    # –ü—Ä–æ–±—É–µ–º —Å–¥–µ–ª–∞—Ç—å URL –ø–æ–ª–Ω—ã–º
                                    full_url = urljoin(driver.current_url, video_url)
                                    if self._is_valid_video_url(full_url):
                                        if full_url not in videos:
                                            videos.append(full_url)
                                            logger.debug(f"üé¨ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤–∏–¥–µ–æ (–ø–æ–ª–Ω—ã–π URL): {full_url}")
                except NoSuchElementException:
                    continue
            
            logger.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ {len(videos)} –≤–∏–¥–µ–æ")
            return videos
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–∏–¥–µ–æ: {e}")
            return []
    
    def _is_valid_image_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not url:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º blob –∏ data URLs
        if url.startswith('blob:') or url.startswith('data:'):
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if any(x in url.lower() for x in ['1x1', 'spacer', 'pixel', 'blank']):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp']
        if any(url.lower().endswith(ext) for ext in image_extensions):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω—ã ABC News
        abc_domains = ['abcnews.go.com', 's.abcnews.com', 'img.abcnews.com', 'cdn.abcnews.com']
        parsed_url = urlparse(url)
        if any(domain in parsed_url.netloc for domain in abc_domains):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if 'image' in url.lower() or 'photo' in url.lower() or 'img' in url.lower():
            return True
        
        return False
    
    def _is_valid_video_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL –≤–∏–¥–µ–æ"""
        if not url:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º blob –∏ data URLs
        if url.startswith('blob:') or url.startswith('data:'):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∏–¥–µ–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        video_extensions = ['.mp4', '.webm', '.ogg', '.mov', '.m3u8', '.ts']
        if any(url.lower().endswith(ext) for ext in video_extensions):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∏–¥–µ–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
        video_platforms = ['youtube.com', 'youtu.be', 'vimeo.com', 'abcnews.go.com']
        parsed_url = urlparse(url)
        if any(platform in parsed_url.netloc for platform in video_platforms):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ
        if 'video' in url.lower() or 'stream' in url.lower():
            return True
        
        return False
    
    def _validate_parsed_content(self, content: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä—Å–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        title = content.get('title', '')
        text_content = content.get('content', '')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if not title or len(title) < 10 or title == "ABC News Article":
            logger.warning("‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        if not text_content or len(text_content) < 50:
            logger.warning("‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç")
            return False
        
        logger.info(f"‚úÖ ABC News –∫–æ–Ω—Ç–µ–Ω—Ç –≤–∞–ª–∏–¥–µ–Ω: title={bool(title)}, content={len(text_content)} —Å–∏–º–≤–æ–ª–æ–≤")
        return True
    
    def extract_media(self, url: str, content: Dict[str, Any]) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ–¥–∏–∞ —Ñ–∞–π–ª—ã –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        return {
            'images': content.get('images', []),
            'videos': content.get('videos', [])
        }
    
    def validate_content(self, content: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        if not self._validate_parsed_content(content):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ–¥–∏–∞
        images = content.get('images', [])
        videos = content.get('videos', [])
        
        if not images and not videos:
            logger.warning("‚ö†Ô∏è ABC News: –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –∏–º–µ–µ—Ç –º–µ–¥–∏–∞, –Ω–æ —ç—Ç–æ –¥–æ–ø—É—Å—Ç–∏–º–æ")
            # –î–ª—è ABC News –º–µ–¥–∏–∞ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã, –Ω–µ –±—Ä–∞–∫—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        
        return True
    
    def _is_valid_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL"""
        if not url:
            return False
        
        parsed = urlparse(url)
        
        if not parsed.netloc:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–æ–º–µ–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        if not any(domain in parsed.netloc for domain in self.supported_domains):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ entryId - –µ—Å–ª–∏ –µ—Å—Ç—å, —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π URL –¥–∞–∂–µ –¥–ª—è live-updates
        query_params = parse_qs(parsed.query)
        has_entry_id = 'entryId' in query_params or 'entryid' in query_params
        
        if has_entry_id:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç entryId)
        excluded_patterns = [
            '/live-updates/',
        ]
        
        for pattern in excluded_patterns:
            if pattern in url.lower():
                return False
        
        return True

